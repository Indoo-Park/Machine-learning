---
title: "Stats 102B - Homework 1"
author: "Indoo Park"
date: "Spring 2019"
output: html_document
---

Problems and questions, Copyright Miles Chen. Do not post or distribute.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Modify this file with your answers and responses.

### Reading:

a. A First Course in Machine Learning [FCML]: Chapter 1

## Part 1: Weighted Least Squares Regression

## Task 1A

Solve exercise 1.11 from page 38 of the textbook.

Hint: define matrix $\mathbf{A}$ be a diagonal matrix of the weights $\alpha_1, ... \alpha_N$ as follows:

$$
\begin{bmatrix}
\alpha_1 & 0 & \ldots & 0 \\
0 & \alpha_2 & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & \alpha_N \\
\end{bmatrix}
$$

Hint: With this matrix defined, the loss function can be written as:

$$\mathcal{L} = \frac{1}{N} (\mathbf{t} - \mathbf{Xw})^T\mathbf{A}(\mathbf{t} - \mathbf{Xw})$$

Suggestion: Do your work with pencil and paper. Write the dimensions underneath each matrix to make sure you have the order of multiplication correct.

As you work to find your solution, typeset the gradient of the loss w.r.t. $\mathbf{w}$

$$\frac{\partial \mathcal{L}}{\partial \mathbf{w}} = \textbf{0} -\frac{2}{N}\mathbf{X}^T\mathbf{A}\mathbf{t} + \frac{2}{N}\mathbf{X}^T\mathbf{A}\mathbf{X}\mathbf{w}= \frac{-2}{N}\mathbf{X}^T\mathbf{A}\mathbf{t}+\frac{2}{N}\mathbf{X}^T\mathbf{A}\mathbf{X}\mathbf{w}$$

Finally, typeset your solution for $\mathbf{\hat{w}}$ here:

$$\mathbf{\hat{w}} = (\mathbf{X}^T\mathbf{A}\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{A}\mathbf{t}$$

If you're new to Latex and typesetting, you can visit: https://en.wikibooks.org/wiki/LaTeX/Mathematics for examples to follow.

## Task 1B

Let's see the effect of altering the values of our weights $\alpha$. 

We'll use this following toy data, where x is the values 1 through 4, and t is the values, 1, 3, 2, 4.

```{r}
x <- c(1,2,3,4)
t <- c(1,3,2,4)
```

- We begin by setting our vector of weights all equal. I have already fit a model using `lm()` with the argument `weights`. Go ahead and use the matrix operations from your solution from above to find and print the parameter estimates. They should equal the parameter estimates found via `lm()`

```{r}
a1 <- c(1,1,1,1)
model1 <- lm(t ~ x, weights = a1)
plot(x,t,xlim = c(0,5), ylim = c(0,5), asp=1)
abline(model1)

print(model1$coefficients)

## your code to make the matrices and find the parameter estimates
X <- cbind(1,x)
X
A<- diag(a1,length(x))
A
w_hat <- solve(t(X) %*%A%*% X)%*% t(X)%*%A %*% t
w_hat
# The parameter estimates are 0.5 for the intercept, and 0.8 for the slope.
```

- Now we alter the weights. This vector puts large weight on the two inner points (x=2, x= 3), and small weight on the outer points (x=1, x=4). Again, use the matrix operations to find and print the parameter estimates using the provided weights. Compare them against the estimates found via `lm()`. I have plotted the fitted line, comment on the effect of the weights.

```{r}
a2 <- c(0.1, 5, 5, 0.1) #
model2 <- lm(t ~ x, weights = a2)
plot(x,t,xlim = c(0,5), ylim = c(0,5), asp=1)
abline(model2)
print(model2$coefficients)
## your code
X <- cbind(1,x)
X
A<- diag(a2,length(x))
A
w_hat <- solve(t(X) %*%A%*% X)%*% t(X)%*%A %*% t
w_hat
#4.24 for the intercept and -.69 for the slople.
#The parameter estimates match with the estimates found via lm()
```

- We alter the weights again. This time large weight are on the two outer points (x=1, x=4), and small weight on the inner points (x=2, x= 3). Again, use the matrix operations to find and print the parameter estimates using the provided weights. Compare them against the estimates found via `lm()`. Look at the fitted line and comment on the effect of the weights.

```{r}
a3 <- c(5, 0.1, 0.1, 5) 
model3 <- lm(t ~ x, weights = a3)
plot(x,t,xlim = c(0,5), ylim = c(0,5), asp=1)
abline(model3)

print(model3$coefficients)

#matrix method
X <- cbind(1,x)
X
A<- diag(a3,length(x))
A
w_hat <- solve(t(X) %*%A%*% X)%*% t(X)%*%A %*% t
w_hat
```

- Try to explain Weighted Least Squares regression. What effect would putting a very large weight, say 1000, on a point have on the regression line?  What effect would putting a weight of 0 on a point have on the regression line?

That putting a larger weight on the point is the giving more influcence to the point in the fitted model. The regression line will go through the point. That putting a weight of 0 on a point is like excluding the observations on that point from the regression analysis. The regression line will be drawn with ignore the point with 0 weight (will no go through the point) .  


## Part 2: OLS Matrix Notation

## Task 2

Review Lecture 1-3. 

We'll take a look at the Chirot dataset which covers the 1907 Romanian Peasant Revolt. General info on the event: https://en.wikipedia.org/wiki/1907_Romanian_Peasants%27_revolt

The data covers 32 counties in Romania, and the response variable is the intensity of the rebellion.

The orginal paper by Daniel Chirot, which provides details of the analysis and variables: https://www.jstor.org/stable/2094430

A basic data dictionary can be found with `help(Chirot)`

```{r}
library(carData)
data(Chirot)
chirot_mat <- as.matrix(Chirot)
```

We'll do an analysis with matrix operations. We start by extracting the commerce column and creating our $\mathbf{X}$ matrix.

```{r}
t <- chirot_mat[, 1, drop = FALSE]  # response, keep as a matrix
x <- chirot_mat[, 2, drop = FALSE]  # commerce column, keep as matrix
X <- cbind(1, x)
colnames(X) <- c('1','commerce')
head(X)
```

- Use `lm()` to fit the rebellion intensity to matrix X (which has columns for a constant and commercialization). Make sure you only calculate the coefficient for the intercept once.

```{r}
# ... your code ...
lm(t~0+X)
```

- Using only matrix operations, calculate and show the coefficient estimates. Verify that they match the estimates from `lm()`.

```{r}
# ... your code ...
w_hat1 <- solve(t(X)%*% X)%*% t(X)%*% t
w_hat1

#-2.81 for the intercept and 0.11 for the slope.
#They match the estimates from 'lm()'
```

- Create another matrix (call it X_ct) with three columns: a constant, variable commerce, variable tradition. Print the head of this matrix.

```{r}
# ... your code ...
X_ct <- cbind(1,Chirot$commerce,Chirot$tradition)
colnames(X_ct) <- c('1','commerce','tradition')
head(X_ct)
```

- Using matrix operations, calculate and show the coefficient estimates of the model with the variables commerce and tradition.

```{r}
# ... your code ...
w_hat2<-solve(t(X_ct)%*%X_ct)%*%t(X_ct)%*%t
w_hat2
```

- Create another matrix (call it X_all) with all of the x variables (plus a constant). Print the head of this matrix. Using matrix operations, calculate and show the coefficient estimates of the model with all the variables.

```{r}
# ... your code ...

X_all <- cbind(1,Chirot$commerce,Chirot$tradition,Chirot$midpeasant,Chirot$inequality)
colnames(X_all)<-c('1','commerce','tradition','midpeasant','inequality')
head(X_all)
w_hat3<-solve(t(X_all)%*%X_all)%*%t(X_all)%*%t
w_hat3
```

-  Using matrix operations, calculate the fitted values for all three models. (No need to print out the fitted values.) Create plots of fitted value vs actual value for each model (there will be three plots). Be sure to provide a title for each plot.

```{r}
# ... your code ...

#calculate the fitted values for all three models.
t_hat1 <- X %*% w_hat1
t_hat2 <- X_ct %*% w_hat2
t_hat3 <- X_all %*% w_hat3

par(mfrow=c(2,2))
plot(t_hat1,t,ylab='Actual',xlab='fitted',main="actual vs fitted value from model1")
plot(t_hat2,t,ylab='Actual',xlab='fitted', main= "actual vs fitted value from model2")
plot(t_hat3,t,ylab='Actual',xlab='fitted',main="actual vs fitted value from model3")

```

- Now that you have calculated the columns of fitted values, find the Residual Sum of Squares and the R-squared values of the three models. Which model has the smallest RSS? (RSS is the sum of (actual - fitted)^2. R-sq is the correlation between actual and fitted, squared.)

```{r}
# ... your code ...
rss1 <- sum((t-t_hat1)^2)
rss1 #RSS of first model
rss2 <- sum((t-t_hat2)^2)
rss2 #RSS of second model
rss3 <- sum((t-t_hat3)^2)
rss3 #RSS of third model


r_sq1 <- (cor(t,t_hat1))^2
r_sq1 #r-squared value of the first model
r_sq2 <- (cor(t,t_hat2))^2
r_sq2 #r-squared value of the second model
r_sq3 <- (cor(t,t_hat3))^2
r_sq3 #r-squared value of the third model
```

## Part 3: Cross-Validation

We can use cross-validation to evaluate the predictive performance of several competing models.

I will have you manually implement leave-one-out cross-validation from scratch first, and then use the built-in function in R.

We will use the dataset `ironslag` from the package `DAAG` (a companion library for the textbook Data Analysis and Graphics in R).

The description of the data is as follows: The iron content of crushed blast-furnace slag can be determined by a chemical test at a laboratory or estimated by a cheaper, quicker magnetic test. These data were collected to investigate the extent to which the results of a chemical test of iron content can be predicted from a magnetic test of iron content, and the nature of the relationship between these quantities. [Hand, D.J., Daly, F., et al. (1993) __A Handbook of Small Data Sets__]

The `ironslag` data has 53 observations, each with two values - the measurement using the chemical test and the measurement from the magnetic test.

We can start by fitting a linear regression model $y_n = w_0 + w_1 x_n + \epsilon_n$. A quick look at the scatterplot seems to indicate that the data may not be linear.

```{r linear_model}
# install.packages("DAAG") # if necessary
library(DAAG)
x <- seq(10,40, .1) # a sequence used to plot lines

L1 <- lm(magnetic ~ chemical, data = ironslag)
plot(ironslag$chemical, ironslag$magnetic, main = "Linear fit", pch = 16)
yhat1 <- L1$coef[1] + L1$coef[2] * x
lines(x, yhat1, lwd = 2, col = "blue")
```

In addition to the linear model, fit the following models that predict the magnetic measurement (Y) from the chemical measurement (X).

Quadratic: $y_n = w_0 + w_1 x_n + w_2 x_n^2 + \epsilon_n$

Exponential: $\log(y_n) = w_0 + w_1 x_n + \epsilon_n$, equivalent to $y_n = \exp(w_0 + w_1 x_n + \epsilon_n)$

log-log: $\log(y_n) = w_0 + w_1 \log(x_n)  + \epsilon_n$

## Task 3A

```{r other_models}
# I've started each of these for you. 
# Your job is to create the plots with fitted lines.

L2 <- lm(magnetic ~ chemical + I(chemical^2), data = ironslag)
plot(ironslag$chemical, ironslag$magnetic, main = "linear fit for quadratic ", pch = 16)
yhat2 <- L2$coef[1] + L2$coef[2] * x + L2$coef[3]*x^2
lines(x, yhat2, lwd = 2, col = "blue")

L3 <- lm(log(magnetic) ~ chemical, data = ironslag)
plot(ironslag$chemical, ironslag$magnetic, main = "linear fit for exponential", pch = 16)
yhat3 <- L3$coef[1] + L3$coef[2] * x # log(y-hat)
yhat3 <- exp(yhat3) #exponentiate log(y-hat)
lines(x, yhat3, lwd = 2, col = "blue")
# when plotting the fitted line for this one, create estimates of log(y-hat) linearly
# then exponentiate log(y-hat)

L4 <- lm(log(magnetic) ~ log(chemical), data = ironslag)
plot(log(ironslag$chemical), log(ironslag$magnetic), main = "linear fit for log-log", pch = 16)
yhat4 <- L4$coef[1] + L4$coef[2] * log(x)  #log(y-hat)
lines(log(x), yhat4, lwd = 2, col = "blue")
# for this one, use plot(log(chemical), log(magnetic))
# the y-axis is now on the log-scale, so you can create and plot log(y-hat) directly
# just remember that you'll use log(x) rather than x directly
```


## Task 3B: Leave-one-out Cross validation

You will now code leave-one-out cross validation. In LOOCV, we remove one data point from our data set. We fit the model to the remaining 52 data points. With this model, we make a prediction for the left-out point. We then compare that prediction to the actual value to calculate the squared error. Once we find the squared error for all 53 points, we can take the mean to get a cross-validation error estimate of that model.

To test out our four models, we will build a loop that will remove one point of data at a time. Thus, we will make a `for(i in 1:53)` loop. For each iteration of the loop, we will fit the four models on the remaining 52 data points, and make a prediction for the remaining point.

```{r loocv}

# create vectors to store the validation errors for each model
# error_model1 <- rep(NA, 53)
# ...

error_model1 <- rep(NA, 53)
error_model2 <- rep(NA, 53)
error_model3 <- rep(NA, 53)
error_model4 <- rep(NA, 53)

for(i in 1:53){
  valid<- ironslag[i,]
  training<-ironslag[-i,]
  
  # write a line to select the ith line in the data
  # store this line as the 'validation' case
  # store the remaining as the 'training' data
  
  model1 <- lm(magnetic ~ chemical, data = training)
  fitted_value <- predict(model1, valid)
  error_model1[i] <- (valid[1,2] - fitted_value)^2
   
  model2 <- lm(magnetic ~ chemical + I(chemical^2), data = training)
  fitted_value <- predict(model2, valid)
  error_model2[i] <- (valid[1,2] - fitted_value)^2

  model3 <- lm(log(magnetic) ~ chemical, data = training)
  fitted_value <- predict(model3,valid)
  error_model3[i] <- (valid[1,2] - exp(fitted_value))^2
  
  model4 <- lm(log(magnetic) ~ log(chemical), data = training)
  fitted_value <- predict(model4, valid)
  error_model4[i] <- (valid[1,2] - exp(fitted_value))^2
    
  # fit the four models and calculate the prediction error for each one
  # hint: it will be in the form
  # model1 <- lm(magnetic ~ chemical, data = training)
  # fitted_value <- predict(model1, test_case)
  # error_model1[i] <- (validation_actual_value - fitted_value)^2
  # ...
  # model2 <- 
  # ...
  # ...
  # for models where you are predicting log(magnetic), you'll want to 
  # exponentiate the fitted value before you compare it to the validation case 
  # error[i] <- (validation_actual_value - exp(fitted_value))^2
  

}


# once all of the errors have been calculated, find the mean squared error
# ...
# mean(error_model1)
mean(error_model1)
mean(error_model2)
mean(error_model3)
mean(error_model4)
```

Compare the sizes of the cross validation error to help you decide which model does the best job of predicting the test cases.

- Model2 has the least error, therefore the model2 (quadratic) is the best model to predict.


## Task 3C: Cross-validation with R

Now that you have written your cross-validation script from scratch, we can use the built-in functions in R. Library(boot) has the function `cv.glm()` which can be used to estimate cross-validation error.

To make use of `cv.glm()` on the linear models, we must first use `glm()` to fit a generalized linear model to our data. If you do not change the attribute "family" in the function `glm()`, it will fit a linear model

```{r cvwithR, error = TRUE}
library(boot)
gL1 <- glm(magnetic ~ chemical, data = ironslag) # equivalent to lm(magnetic ~ chemical)
loocv1 <- cv.glm(ironslag, gL1)$delta[1]
loocv1

gL2 <- glm(magnetic ~ chemical + I(chemical^2),data = ironslag)
loocv2 <- cv.glm(ironslag, gL2)$delta[1]
loocv2

gL3 <-glm(log(magnetic) ~ chemical, data = ironslag)
loocv3<- cv.glm(ironslag, gL3,cost = function(magnetic,yhat3)(exp(magnetic)-exp(yhat3))^2)$delta[1]
loocv3

gL4 <- glm(log(magnetic) ~ log(chemical),data=ironslag)
loocv4 <- cv.glm(ironslag, gL4, cost = function(magnetic,yhat4)(exp(magnetic)-exp(yhat4))^2)$delta[1]
loocv4
# find the LOOCV CV values for all of the models
# for the models with log(magnetic), use the argument cost to specify your own 
# cost function: cost = function(y, yhat) (exp(y) - exp(yhat))^2

```

Your LOOCV estimates from `cv.glm()` should match your estimates when you coded your algorithm from scratch.


Based on your Cross Validation scores, which model seems to be the best?

Model2 seems to be the best as like the result from manual LOOCV.

## Task 3D: Revisit Chirot

Use `glm()` to fit the three models to the Chirot data. 

- model1: intensity ~ commerce
- model2: intensity ~ commerce + tradition
- model3: intensity ~ all other variables

Use `cv.glm()` to calculate the LOOCV scores to compare models. Which model would you select?

```{r}
# ... your code ...
gL1 <- glm(intensity ~ commerce, data = Chirot) # equivalent to lm(magnetic ~ chemical)
loocv1 <- cv.glm(Chirot, gL1)$delta[1]
loocv1
gL2 <- glm(intensity~commerce+ tradition, data = Chirot)
loocv2 <- cv.glm(Chirot, gL2)$delta[1]
loocv2
gL3 <-glm(intensity ~ commerce+ tradition+midpeasant+inequality, data = Chirot)
loocv3<- cv.glm(Chirot, gL3)$delta[1]
loocv3
```

-Since the model2 has the least loocv score, I will select the model2.


## Part 4: Using R's `optim()` function

The `optim()` function in R is quite versatile and can be used to minimize a wide range of functions using a variety of methods. The default method, the Nelder-Mead simplex algorithm, is a numeric method that works quite well for multidimensional functions. Other methods, like BFGS which uses a modified version of gradient descent, are included and are also quite powerful.

We will do an exercises to use `optim()` in to minimize the loss function in linear regression.

We'll use a synthetic data set.

```{r}
x <- 1:100
set.seed(1)
e <- rnorm(100, 0, 40)
t <- 30 + 3.4 * x + e   # the true slope is 3.4, the true intercept is 30
coefs <- coef(lm(t~x)) # the coefficients from using lm
print(coefs)  #if you use set.seed(1), this should be 35.266629, 3.381958
plot(x,t)
abline(coef = coefs, col = "red")
```

## Task 4: Use `optim()` to minimize the sum of squared residuals in linear regression

For the first task, we define the loss function as the sum of squared residuals:

$$\mathcal{L} = F(w_0, w_1) = \sum_{n=1}^{N}{(t_n - \hat{t}_n)^2}$$

Where $hat{t}$ is the predicted value from the linear model: $t_i = w_0 + w_1 x_i$

We can also use matrix notation:

$$\mathcal{L} = F(\mathbf{w}) = (\mathbf{t} - \mathbf{Xw})^T(\mathbf{t} - \mathbf{Xw})$$

Write your loss function as a function of the vector `par` which will have a lenght of 2. The vector par will contain the current estimates of the intercept and slope. Treat the data (`x` and `t`) as fixed quantities that are available in the global environment. 

The function will return a single value: the sum of squared residuals.

```{r}
loss <- function(par){
  t_hat<-par[1]+par[2]*x
  rss<-sum((t-t_hat)^2)
  return(rss)
  # use x, and par to create t-hat values
  # take the difference between t and t-hat and sum the squares
  # return the total sum of squared residuals
  
  
  # ...or... create the necessary matrices and compute the total sum of squares
}
```

The following chunk runs optim and returns the results. The arguments it takes in is the initial values of `par`, and the function that it is trying to minimize. I also specify the method 'BFGS' which uses a version of gradient descent as the numeric algorithm to optimize the function.

```{r, error = TRUE}
results <- optim(par = c(0,1), fn = loss, method = 'BFGS')
results$par
```

When we look at the results and the values of the parameters that minimize the loss function, they should match the coefficients estimated by `lm()`.

